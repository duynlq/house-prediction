---
output:
  pdf_document: default
  html_document: default
---
What determines the value of a house? 

```{r setup, ALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=F, message=F}
####Libaries####

library(tidyverse)
library(ggplot2)
library(caret)     #createDataPartition
library(DAAG)      #CVlm
library(car)       #leverage.plots
library(lindia)    #gg_cooksd
library(gridExtra) #grid.arrange
library(kableExtra)
library(olsrr)     #ols_step_forward_aic

```

```{r }
####Import Data####

getwd()
train <- read.csv("train.csv")
test <- read.csv("test.csv")
```

```{r }
####Analysis 1 Wrangle Data####
df <- dplyr::filter(train, Neighborhood =="Edwards" | Neighborhood =="NAmes" | 
                      Neighborhood == "BrkSide")
df$Neighborhood <- as.factor(df$Neighborhood)
str(df)
```

<font size=6 color="Green" family="Courier"><b>
Raw vs Log-transformed Scatterplots</b></font>

```{r, }
####Analysis 1 EDA####

#Normal Data Scatterplot
ggplot(df, aes(x = GrLivArea, y = SalePrice, color = Neighborhood)) +
  geom_point() +
  ggtitle("Raw Data")

#Log-transformed Data Scatterplot
ggplot(df, aes(x = log(GrLivArea), y = log(SalePrice), color = Neighborhood)) +
  geom_point() +
  ggtitle("Log-transformed Data")
```

<font size=6 color="Green" family="Courier"><b>
How does grade living area relate to sales price?</b></font>

From an initial model, we can deduce from the R-Squared values below that, the log-transformed grade living area is about 42% effective at explaining log-transformed sales price of a house.

$$log(SalePrice) = B0 + B1log(GrLivArea)$$

```{r, }
model0 <- lm(log(SalePrice) ~ log(GrLivArea), data = df)
paste(summary(model0)$r.squared, "  | ", summary(model0)$adj.r.squared)
```

We can further hone our dataset with influential points analysis.

```{r, }
#Cook's Distance Plot
gg_cooksd(model0)
```

```{r, }
#Identify influential points 
#From the above graph, those points are greater than 0.1 Cook's D
as.numeric(names(cooks.distance(model0))[(cooks.distance(model0) > 0.1)])
#And they are 131, 136, 339

#Start removing outliers one by one and a combo of outliers
#And find the most desired R-Squared and Adj R-Squared
A1Data01 = df[-131,]
model01 <- lm(log(SalePrice) ~ log(GrLivArea), data = A1Data01)
paste(summary(model01)$r.squared, "  | ", summary(model01)$adj.r.squared)

A1Data02 = df[-136,]
model02 <- lm(log(SalePrice) ~ log(GrLivArea), data = A1Data02)
paste(summary(model02)$r.squared, "  | ", summary(model02)$adj.r.squared)

A1Data03 = df[-339,]
model03 <- lm(log(SalePrice) ~ log(GrLivArea), data = A1Data03)
paste(summary(model03)$r.squared, "  | ", summary(model03)$adj.r.squared)

A1Data04 = df[-c(131, 136, 339),]
model04 <- lm(log(SalePrice) ~ log(GrLivArea), data = A1Data04)
paste(summary(model04)$r.squared, "  | ", summary(model04)$adj.r.squared)

A1Data05 = df[-c(131, 136),]
model05 <- lm(log(SalePrice) ~ log(GrLivArea), data = A1Data05)
paste(summary(model05)$r.squared, "  | ", summary(model05)$adj.r.squared)

A1Data06 = df[-c(131, 339),]
model06 <- lm(log(SalePrice) ~ log(GrLivArea), data = A1Data06)

#The most desired is A1Data06 and model06
A1Data = A1Data06
model1 = model06

#str(A1Data)
#str(model1)
```

Observations 131 and 136 are then removed from the dataset.

The R-Square values of our new model proves that it performs slighty better, 
with grade living area effectively explaining 44% of sales prices of houses.

```{r, }
paste(summary(model06)$r.squared, "  | ", summary(model06)$adj.r.squared)
```

The scatters on our new plot now appears more normalized, judging by the arbitrary diagonal red line.

```{r, message=F}
#Log-transformed Data Scatterplot without indexes 131 and 339
ggplot(A1Data, aes(x = log(GrLivArea), y = log(SalePrice), color = Neighborhood)) +
  geom_point() +
  annotate(geom = "segment", x = 5.75, y = 10.5, xend = 8.15, yend = 12.75, color = "red") +
  ggtitle("Log-transformed Data ScatterLot -c(131, 339)")

```

<font size=6 color="Green" family="Courier"><b>
What if neighborhoods are included?</b></font>

The R-Squared values below proves that our full model performs the best,
with Neighborhood added as an additive.

$$log(SalePrice) = B0 + B1log(GrLivArea) + B2Edwards + $$
$$B3NAmes + B4Edwards*log(GrLivArea) + B5NAmes*log(GrLivArea)$$

```{r, }
####Model 2 Modeling####
model21 <- lm(log(SalePrice) ~ log(GrLivArea) + as.factor(Neighborhood), data = A1Data)
paste(summary(model21)$r.squared, "  | ", summary(model21)$adj.r.squared)
#"BrkSide" is read first so it was used as reference

model22 <- lm(log(SalePrice) ~ log(GrLivArea) + as.factor(Neighborhood) + 
                as.factor(Neighborhood)*log(GrLivArea), data = A1Data)
#model22 <- lm(log(SalePrice) ~ log(GrLivArea) * as.factor(Neighborhood), data = A1Data)
paste(summary(model22)$r.squared, "  | ", summary(model22)$adj.r.squared)
press(model22)
```

```{r, }
#The most desired is the full model
model2 = model22
paste(summary(model2)$r.squared, "  | ", summary(model2)$adj.r.squared)
#press(model2)
```

<font size=6 color="Green" family="Courier"><b>
Final Model Assumptions</b></font>

```{r, message=F}
####Model 2 EDA####

#Residuals QQ Plot
residuals = resid(model2)
p1 = ggplot(A1Data, aes(sample = residuals)) +
  geom_qq() +
  geom_qq_line(color = "red") +
  labs(title = "QQ Plot of Residuals", x = "Theoretical Quantile", y = "Actual Quantile")

#Residuals Histogram
p2 = ggplot(A1Data, aes(residuals)) +
  geom_histogram(aes(y = ..density..)) +
  geom_density(alpha = .2, color = "red", fill = "azure") +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Density")

#Cook's Distance Plot
library(lindia)
p3 = gg_cooksd(model2)

stdres2 <- rstandard(model2)
#Neighborhood vs RStudent
p4 = ggplot(A1Data, aes(as.factor(Neighborhood), stdres2)) + 
  geom_boxplot() +
  labs(title = "    RStudent Boxplot", x = "Neighborhood", y = "RStudent")

#Standardized Residuals Plot
p5 = ggplot(A1Data, aes(x = seq(stdres2), y = stdres2)) +
  geom_point() +
  geom_hline(yintercept = 3, color = "red") +
  geom_hline(yintercept = -3, color = "red") +
  labs(title = "Prediction vs RStudent", x = "Predicted Value", y = "RStudent")

#Standardized Residuals vs Leverage
p6 = gg_resleverage(model2, method = "loess", se = FALSE, scale.factor = 1) +
  labs(title = "Leverage vs RStudent", x = "Leverage", y = "RStudent")

grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 3)
```

<font size=6 color="Green" family="Courier"><b>
Sanity Check of Above 2 Models</b></font>

### Extra Sums of Square Test

There is enough evidence below to conclude that the latter model is superior (p-value < 0.05).

```{r, }
####Last defense between competing models####

#Extra sums of square test
anova(model1, model2)
```

### Repeated Cross-Validation

Both models are evaluated 5 times using RMSE, R-Squared, and PRESS, respectively.
We desire the lower RMSE, higher R-Squared, and lower PRESS.
The latter model qualifies all three categories.

```{r, }

#Repeated Cross-validation
set.seed(760397) 
train.control <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

#Train CV model1
CVModel1 <- train(log(SalePrice) ~ log(GrLivArea), 
                  data = A1Data, method = 'lm',
                  trControl = train.control)
paste(CVModel1$results$RMSE, "  | ", CVModel1$results$Rsquared, " | ", press(model1))

#Train CV model1
CVModel2 <- train(log(SalePrice) ~ log(GrLivArea) + as.factor(Neighborhood) + as.factor(Neighborhood)*log(GrLivArea), 
                  data = A1Data, method = 'lm',
                  trControl = train.control)
paste(CVModel2$results$RMSE, "  | ", CVModel2$results$Rsquared, " | ", press(model2))
```

```{r, }
#Parameters
summary(model2)
confint(model2)
```