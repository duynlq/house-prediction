---
title: "MSDS 6371 Project Analysis 2"
author: "Duy Nguyen"
date: "4/8/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
####Libaries####

library(tidyverse)
library(ggplot2)
library(caret)     #createDataPartition
library(DAAG)      #CVlm
library(car)       #leverage.plots
library(lindia)    #gg_cooksd
library(gridExtra) #grid.arrange
library(kableExtra)
# library(readr)
# library(purrr)
# library(forcats)
# library(imputeMissings)
# library(leaps)
library(MASS)      #stepAIC()
library(olsrr)     #ols_step_forward_aic
library(forcats)   #fct_rev
library(Metrics)   #rmse
# library(asbio)
```

```{r}
####Import Data####
getwd()
train <- read.csv("train.csv")
test <- read.csv("test.csv")
```

```{r}
####Wrangling Data####
df_train = train
df_test = test

dim(df_train)
dim(df_test)
```

```{r}
####train: Dealing with NAs####
prop.table(table(is.na(df_train)))
colSums(is.na(df_train))
names(df_train)[sapply(df_train, anyNA)]

#We will now go down the list

#LotFrontage
prop.table(table(df_train$LotFrontage, useNA = "ifany"))
ggplot(df_train, aes(x = LotFrontage)) + geom_density() 
#LotFrontage is heavily right skewed, therefore its NAs are imputed with its median
df_train$LotFrontage[is.na(df_train$LotFrontage)] <- mean(df_train$LotFrontage, na.rm=TRUE)

#Alley
prop.table(table(df_train$Alley, useNA = "ifany"))
df_train$Alley[is.na(df_train$Alley)] <- "None"
#test$Alley[is.na(test$Alley)] <- "None"

#MasVnrArea
prop.table(table(df_train$MasVnrArea, useNA = "ifany"))
#MasVnrType 
prop.table(table(df_train$MasVnrType, useNA = "ifany"))
#An NA MasVnrArea seems to tie with an NA MasVnrType, therefore its NAs are set to 0
df_train$MasVnrType[is.na(df_train$MasVnrType)] <- "None"
df_train$MasVnrArea[is.na(df_train$MasVnrArea)] <- 0

################################################################################
table(df_train$BsmtQual, useNA = "ifany")
table(df_train$BsmtCond, useNA = "ifany")

#BsmtQual
prop.table(table(df_train$BsmtQual, useNA = "ifany"))
df_train$BsmtQual[is.na(df_train$BsmtQual)] <- "None"

#BsmtCond
prop.table(table(df_train$BsmtCond, useNA = "ifany"))
df_train$BsmtCond[is.na(df_train$BsmtCond)] <- "None"
################################################################################

#BsmtExposure
prop.table(table(df_train$BsmtExposure, useNA = "ifany"))
df_train$BsmtExposure[is.na(df_train$BsmtExposure)] <- "None"

#BsmtFinType1
prop.table(table(df_train$BsmtFinType1, useNA = "ifany"))
df_train$BsmtFinType1[is.na(df_train$BsmtFinType1)] <- "None"

#BsmtFinType2
prop.table(table(df_train$BsmtFinType2, useNA = "ifany"))
df_train$BsmtFinType2[is.na(df_train$BsmtFinType2)] <- "None"

#Electrical
prop.table(table(df_train$Electrical, useNA = "ifany"))
df_train$Electrical[is.na(df_train$Electrical)] <- "None"

#FireplaceQu
prop.table(table(df_train$FireplaceQu, useNA = "ifany"))
df_train$FireplaceQu[is.na(df_train$FireplaceQu)] <- "None"

#GarageType
prop.table(table(df_train$GarageType, useNA = "ifany"))
df_train$GarageType[is.na(df_train$GarageType)] <- "None"

#GarageYrBlt
prop.table(table(df_train$GarageYrBlt, useNA = "ifany"))
#An NA Garage Year Built seems to tie with an NA Garage Type, therefore its NAs are set to 0
#No need to address GarageType NAs since the variable is not numeric
df_train$GarageYrBlt[is.na(df_train$GarageYrBlt)] <- 0

#GarageFinish
prop.table(table(df_train$GarageFinish, useNA = "ifany"))
df_train$GarageFinish[is.na(df_train$GarageFinish)] <- "None"

################################################################################
table(df_train$GarageQual, useNA = "ifany")
table(df_train$GarageCond, useNA = "ifany")

#GarageQual
prop.table(table(df_train$GarageQual, useNA = "ifany"))
df_train$GarageQual[is.na(df_train$GarageQual)] <- "None"

#GarageCond
prop.table(table(df_train$GarageCond, useNA = "ifany"))
df_train$GarageCond[is.na(df_train$GarageCond)] <- "None"
################################################################################

#PoolQC
prop.table(table(df_train$PoolQC, useNA = "ifany"))
df_train$PoolQC[is.na(df_train$PoolQC)] <- "None"

#Fence
prop.table(table(df_train$Fence, useNA = "ifany"))
df_train$Fence[is.na(df_train$Fence)] <- "None"

#MiscFeature
prop.table(table(df_train$MiscFeature, useNA = "ifany"))
df_train$MiscFeature[is.na(df_train$MiscFeature)] <- "None"

colSums(is.na(df_train))
```

```{r}
####train: Factoring columns according to data_description.txt####

#Seeking out numeric and non-numeric columns
str(df_train)
names(df_train)[sapply(df_train, is.numeric)]
names(df_train)[sapply(df_train, is.character)]

#Character variables into factors
df_train[sapply(df_train, is.character)] <- lapply(df_train[sapply(df_train, is.character)], as.factor)
#This also applies for columns with both chars and nums, which is still 
#consistent with the actual meaning of such columns that are meant for factoring

#Sanity check
str(df_train[sapply(df_train, is.character)])
str(df_train[sapply(df_train, is.numeric)])
#Numeric columns at this point mistakenly includes obscure factor-able columns
#like MSSubClass, OverallQual, OverallCond
#Let's try and fix that

#MSSubClass
df_train$MSSubClass <- as.factor(df_train$MSSubClass)
str(df_train$MSSubClass)
table(df_train$MSSubClass)

#OverallQual
df_train$OverallQual <- as.factor(df_train$OverallQual)
str(df_train$OverallQual)

#OverallCond
df_train$OverallCond <- as.factor(df_train$OverallCond)
str(df_train$OverallCond)

#Sanity check 2
str(df_train[sapply(df_train, is.numeric)])
str(df_train[sapply(df_train, is.factor)])
```

```{r}
####train: Removing Columns####

#Redundant means a value representing over 99% of the data in a column 

#Utitilies
prop.table(table(df_train$Utilities, useNA = "ifany"))#["1"]

#PoolArea
prop.table(table(df_train$PoolArea, useNA = "ifany"))

df_train[,c("Utilities", "PoolArea", "GrLivArea", "TotalBsmtSF")] <- list(NULL)
```

```{r}
####train: EDA####
model0 = lm(log(SalePrice)~., data = df_train)
paste(summary(model0)$r.squared, "  | ", summary(model0)$adj.r.squared)
#         R-Squared = 0.944680793010884
#Adjusted R-Squared = 0.932797066613555"

#Residuals QQ Plot
residuals = resid(model0)
p1 = ggplot(df_train, aes(sample = residuals)) +
  geom_qq() +
  geom_qq_line(color = "red") +
  labs(title = "QQ Plot of Residuals", x = "Theoretical Quantile", y = "Actual Quantile")

#Residuals Histogram
p2 = ggplot(df_train, aes(residuals)) +
  geom_histogram(aes(y = ..density..), bins = 30) +
  geom_density(alpha = .2, color = "red", fill = "azure") +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Density")

#Cook's Distance Plot
library(lindia)
p3 = gg_cooksd(model0)

#Neighborhood vs RStudent
stdres2 <- rstandard(model0)
p4 = ggplot(df_train, aes(as.factor(Neighborhood), stdres2)) + 
  geom_boxplot() +
  labs(title = "    RStudent Boxplot", x = "Neighborhood", y = "RStudent")

#Standardized Residuals Plot
p5 = ggplot(df_train, aes(x = seq(stdres2), y = stdres2)) +
  geom_point() +
  geom_hline(yintercept = 3, color = "red") +
  geom_hline(yintercept = -3, color = "red") +
  labs(title = "Prediction vs RStudent", x = "Predicted Value", y = "RStudent")

#DFFITS
p6 = ggplot(df_train, aes(x = seq(dffits(model0)), y = dffits(model0))) + 
  geom_point() + 
  geom_hline(color="red", yintercept=0) + 
  labs(title = "DFFITS", x = "Observation Number", y = "DFFITS")
  ylim(-5,5)

grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 3)

#Standardized Residuals vs Leverage
plot(model0, which = 5)
```

```{r}
####train: Modeling with outliers####

#Identify influential points with Cook's D > 0.1
as.numeric(names(cooks.distance(model0))[(cooks.distance(model0) > 0.1)])
#And they are 89, 524, 826

paste(summary(model0)$r.squared, " | ", summary(model0)$adj.r.squared)

#Start removing outliers one by one and a combo of outliers
#And find the most desired R-Squared and Adj R-Squared
A2Data01 = df_train[-89,]
model01 <- lm(log(SalePrice)~., data = A2Data01)
paste(summary(model01)$r.squared, " | ", summary(model01)$adj.r.squared)

A2Data02 = df_train[-524,]
model02 <- lm(log(SalePrice)~., data = A2Data02)
paste(summary(model02)$r.squared, " | ", summary(model02)$adj.r.squared)

A2Data03 = df_train[-826,]
model03 <- lm(log(SalePrice)~., data = A2Data03)
paste(summary(model03)$r.squared, " | ", summary(model03)$adj.r.squared)

A2Data04 = df_train[-c(89, 524, 826),]
model04 <- lm(log(SalePrice)~., data = A2Data04)
paste(summary(model04)$r.squared, " | ", summary(model04)$adj.r.squared)

A2Data05 = df_train[-c(89, 524),]
model05 <- lm(log(SalePrice)~., data = A2Data05)
paste(summary(model05)$r.squared, " | ", summary(model05)$adj.r.squared)

A2Data06 = df_train[-c(89, 826),]
model06 <- lm(log(SalePrice)~., data = A2Data06)
paste(summary(model06)$r.squared, " | ", summary(model06)$adj.r.squared)

A2Data07 = df_train[-c(524, 826),]
model07 <- lm(log(SalePrice)~., data = A2Data07)
paste(summary(model07)$r.squared, " | ", summary(model07)$adj.r.squared)

#The most desired is A2Data05 and model05
A2Data = A2Data05
model1 = model05
paste(summary(model1)$r.squared, "  | ", summary(model1)$adj.r.squared)
#         R-Squared = 0.952373329054498
#Adjusted R-Squared = 0.941193169857969
```

```{r}
####test: Dealing with NAs####
table(is.na(df_test))
colSums(is.na(df_test))
names(df_test)[sapply(df_test, anyNA)]
str(df_test)

#We will now go down the list

#MSZoning
prop.table(table(df_test$MSZoning, useNA = "ifany"))
df_test$MSZoning[is.na(df_test$MSZoning)] <- "None"
prop.table(table(df_test$MSZoning, useNA = "ifany"))

#LotFrontage
prop.table(table(df_test$LotFrontage, useNA = "ifany"))
ggplot(df_test, aes(x = LotFrontage)) + geom_density() 
#LotFrontage is heavily right skewed, therefore its NAs are imputed with its median
df_test$LotFrontage[is.na(df_test$LotFrontage)] <- mean(df_test$LotFrontage, na.rm=TRUE)
ggplot(df_test, aes(x = LotFrontage)) + geom_density() 

#Alley
prop.table(table(df_test$Alley, useNA = "ifany"))
df_test$Alley[is.na(df_test$Alley)] <- "None"

#Utilities
prop.table(table(df_test$Utilities, useNA = "ifany"))
df_test$Utilities[is.na(df_test$Utilities)] <- "None"

#Exterior1st
prop.table(table(df_test$Exterior1st, useNA = "ifany"))
df_test$Exterior1st[is.na(df_test$Exterior1st)] <- "None"

#Exterior2nd
prop.table(table(df_test$Exterior2nd, useNA = "ifany"))
df_test$Exterior2nd[is.na(df_test$Exterior2nd)] <- "None"

#MasVnrArea
prop.table(table(df_test$MasVnrArea, useNA = "ifany"))
#MasVnrType 
prop.table(table(df_test$MasVnrType, useNA = "ifany"))
#An NA MasVnrArea seems to tie with an NA MasVnrType, therefore its NAs are set to 0
df_test$MasVnrType[is.na(df_test$MasVnrType)] <- "None"
df_test$MasVnrArea[is.na(df_test$MasVnrArea)] <- 0

################################################################################
table(df_test$BsmtQual, useNA = "ifany")
table(df_test$BsmtCond, useNA = "ifany")

#BsmtQual
prop.table(table(df_test$BsmtQual, useNA = "ifany"))
df_test$BsmtQual[is.na(df_test$BsmtQual)] <- "None"

#BsmtCond
prop.table(table(df_test$BsmtCond, useNA = "ifany"))
df_test$BsmtCond[is.na(df_test$BsmtCond)] <- "None"
################################################################################

#BsmtExposure
prop.table(table(df_test$BsmtExposure, useNA = "ifany"))
df_test$BsmtExposure[is.na(df_test$BsmtExposure)] <- "None"

#BsmtFinType1
prop.table(table(df_test$BsmtFinType1, useNA = "ifany"))
df_test$BsmtFinType1[is.na(df_test$BsmtFinType1)] <- "None"

#BsmtFinSF1
prop.table(table(df_test$BsmtFinSF1, useNA = "ifany"))
df_test$BsmtFinSF1[is.na(df_test$BsmtFinSF1)] <- 0

#BsmtFinType2
prop.table(table(df_test$BsmtFinType2, useNA = "ifany"))
df_test$BsmtFinType2[is.na(df_test$BsmtFinType2)] <- "None"

#BsmtFinSF2
prop.table(table(df_test$BsmtFinSF2, useNA = "ifany"))
df_test$BsmtFinSF2[is.na(df_test$BsmtFinSF2)] <- 0

#BsmtUnfSF
prop.table(table(df_test$BsmtUnfSF, useNA = "ifany"))
df_test$BsmtUnfSF[is.na(df_test$BsmtUnfSF)] <- 0

#TotalBsmtSF
prop.table(table(df_test$TotalBsmtSF, useNA = "ifany"))
df_test$TotalBsmtSF[is.na(df_test$TotalBsmtSF)] <- 0

#BsmtFullBath
prop.table(table(df_test$BsmtFullBath, useNA = "ifany"))
df_test$BsmtFullBath[is.na(df_test$BsmtFullBath)] <- 0

#BsmtHalfBath
prop.table(table(df_test$BsmtHalfBath, useNA = "ifany"))
df_test$BsmtHalfBath[is.na(df_test$BsmtHalfBath)] <- 0

#KitchenQual
prop.table(table(df_test$KitchenQual, useNA = "ifany"))
df_test$KitchenQual[is.na(df_test$KitchenQual)] <- "None"

#Functional
prop.table(table(df_test$Functional, useNA = "ifany"))
df_test$Functional[is.na(df_test$Functional)] <- "None"

#FireplaceQu
prop.table(table(df_test$FireplaceQu, useNA = "ifany"))
df_test$FireplaceQu[is.na(df_test$FireplaceQu)] <- "None"

#GarageType
prop.table(table(df_test$GarageType, useNA = "ifany"))
df_test$GarageType[is.na(df_test$GarageType)] <- "None"

#GarageYrBlt
prop.table(table(df_test$GarageYrBlt, useNA = "ifany"))
#An NA Garage Year Built seems to tie with an NA Garage Type, therefore its NAs are set to 0
#No need to address GarageType NAs since the variable is not numeric
df_test$GarageYrBlt[is.na(df_test$GarageYrBlt)] <- 0

#GarageFinish
prop.table(table(df_test$GarageFinish, useNA = "ifany"))
df_test$GarageFinish[is.na(df_test$GarageFinish)] <- "None"

#GarageCars
prop.table(table(df_test$GarageCars, useNA = "ifany"))
df_test$GarageCars[is.na(df_test$GarageCars)] <- 0

#GarageArea
prop.table(table(df_test$GarageArea, useNA = "ifany"))
df_test$GarageArea[is.na(df_test$GarageArea)] <- 0

################################################################################
table(df_test$GarageQual, useNA = "ifany")
table(df_test$GarageCond, useNA = "ifany")

#GarageQual
prop.table(table(df_test$GarageQual, useNA = "ifany"))
df_test$GarageQual[is.na(df_test$GarageQual)] <- "None"

#GarageCond
prop.table(table(df_test$GarageCond, useNA = "ifany"))
df_test$GarageCond[is.na(df_test$GarageCond)] <- "None"
################################################################################

#PoolQC
prop.table(table(df_test$PoolQC, useNA = "ifany"))
df_test$PoolQC[is.na(df_test$PoolQC)] <- "None"

#Fence
prop.table(table(df_test$Fence, useNA = "ifany"))
df_test$Fence[is.na(df_test$Fence)] <- "None"

#MiscFeature
prop.table(table(df_test$MiscFeature, useNA = "ifany"))
df_test$MiscFeature[is.na(df_test$MiscFeature)] <- "None"

#SaleType
prop.table(table(df_test$SaleType, useNA = "ifany"))
df_test$SaleType[is.na(df_test$SaleType)] <- "Oth"

colSums(is.na(df_test))
table(is.na(df_test))
```

```{r}
####test: Factoring columns according to data_description.txt####

#Seeking out numeric and non-numeric columns
str(df_test)
names(df_test)[sapply(df_test, is.numeric)]
names(df_test)[sapply(df_test, is.character)]

#Character variables into factors
df_test[sapply(df_test, is.character)] <- lapply(df_test[sapply(df_test, is.character)], as.factor)
#This also applies for columns with both chars and nums, which is still 
#consistent with the actual meaning of such columns that are meant for factoring

#Sanity check
str(df_test[sapply(df_test, is.character)])
str(df_test[sapply(df_test, is.numeric)])
str(df_test[sapply(df_test, is.factor)])
#Numeric columns at this point mistakenly includes obscure factor-able columns
#like MSSubClass, OverallQual, OverallCond
#Let's try and fix that

#MSSubClass
df_test$MSSubClass <- as.factor(df_test$MSSubClass)
str(df_test$MSSubClass)
table(df_test$MSSubClass)

#OverallQual
df_test$OverallQual <- as.factor(df_test$OverallQual)
str(df_test$OverallQual)

#OverallCond
df_test$OverallCond <- as.factor(df_test$OverallCond)
str(df_test$OverallCond)

#Sanity check 2
str(df_test[sapply(df_test, is.numeric)])
str(df_test[sapply(df_test, is.factor)])
str(df_test)
```

```{r}
####test: Removing Columns####

#Redundant means a value representing over 99% of the data in a column 

#Utitilies
prop.table(table(df_test$Utilities, useNA = "ifany"))#["1"]

#PoolArea
prop.table(table(df_test$PoolArea, useNA = "ifany"))

df_test[,c("Utilities", "PoolArea", "GrLivArea", "TotalBsmtSF")] <- list(NULL)
```

```{r}
####Row-bind the train dataset with the test dataset####

#Make new isTrain column with 0/1

A2Data$isTrain = TRUE
df_test$isTrain = FALSE

dim(A2Data)
dim(df_test)

#Fill in all NAs for SalePrice in df_test
df_test$SalePrice = rep(NA,1459)
str(df_test$SalePrice)

# Combining the train and test set
A2Data_final = rbind(A2Data,df_test)

#df_test$MSZoning <- factor(df_test$MSZoning, levels = levels(A2Data$MSZoning))

#table(A2Data$MSZoning, useNA = "ifany")
#table(df_test$MSZoning, useNA = "ifany")

A2Data = A2Data_final[A2Data_final$isTrain == TRUE,]
df_test = A2Data_final[A2Data_final$isTrain == FALSE,]

dim(A2Data)
dim(df_test)
str(df_test)

# #MSZoning
# df_test<-A2Data[!(A2Data$MSZoning == "None")]
# prop.table(table(df_test$MSZoning, useNA = "ifany"))
# 
# #MSSubClass
# df_test<-A2Data[!(A2Data$MSSubClass == 150)]
# prop.table(table(df_test$MSSubClass, useNA = "ifany"))
# 
# #Functional
# df_test<-A2Data[!(A2Data$Functional == "None")]
# prop.table(table(df_test$Functional, useNA = "ifany"))
```

```{r}
####Training a 70% partition with the train dataset for the test dataset####
set.seed(760397)
index = createDataPartition(y = A2Data$SalePrice, p = 0.7, list = FALSE)
train  = A2Data[index, ]
test = A2Data[-index, ]

```

## Forward Selection

```{r}
#forward.vars = stepAIC(model1, direction = "backward", trace = FALSE)
#forward.vars$anova

model_forward = lm(log(SalePrice) ~ MSSubClass + MSZoning + LotFrontage + LotArea + 
     Street + LotConfig + LandSlope + Neighborhood + Condition1 + 
     Condition2 + OverallQual + OverallCond + YearBuilt + YearRemodAdd + 
     RoofStyle + RoofMatl + Exterior1st + MasVnrType + ExterCond + 
     Foundation + BsmtExposure + BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF + 
     Heating + HeatingQC + CentralAir + X1stFlrSF + X2ndFlrSF + 
     BsmtFullBath + FullBath + HalfBath + KitchenAbvGr + KitchenQual + 
     Functional + Fireplaces + GarageCars + GarageArea + GarageQual + 
     GarageCond + WoodDeckSF + OpenPorchSF + EnclosedPorch + X3SsnPorch + 
     ScreenPorch + PoolQC + Fence + SaleCondition,
                   data = A2Data)
paste(summary(model_forward)$r.squared, " | ", summary(model_forward)$adj.r.squared)
#         R-Squared = 0.948000722728401
#Adjusted R-Squared = 0.940717568869546
press(model_forward)

################################################################################

# model_forward = lm(log(SalePrice)~OverallQual+Neighborhood+TotRmsAbvGrd+
#                      GarageArea+BsmtFinType1+X1stFlrSF+X2ndFlrSF+RoofMatl+
#                      OverallCond+YearBuilt+SaleCondition+BsmtFinSF1+BldgType+
#                      LotArea+Functional+ScreenPorch+CentralAir+Condition1+
#                      KitchenQual+BsmtExposure+HeatingQC+Heating+LandSlope+
#                      Fireplaces+BsmtFullBath+Street+WoodDeckSF+Foundation+
#                      LotConfig+GarageCars+PoolQC+HalfBath+FullBath+KitchenAbvGr+
#                      BsmtUnfSF+BsmtFinSF2+LotFrontage+YearRemodAdd+GarageQual+
#                      Electrical+EnclosedPorch+SaleType+OpenPorchSF+X3SsnPorch+
#                      GarageCond,
#                    data = A2Data)
# summary(model_forward)
# paste(summary(model_forward)$r.squared, " | ", summary(model_forward)$adj.r.squared)
# #         R-Squared = 0.948000722728401
# #Adjusted R-Squared = 0.940717568869546
# press(model_forward)

#predict(model_forward, newdata = df_test)
```

```{r, eval=FALSE, echo=FALSE}
#Doing the prediction on partitioned test set
prediction = predict(model_forward, newdata = df_test)

#Performing inverse log transform
value = exp(prediction)

# Checking the RMSE of the model
model_RMSE = rmse(df_test$SalePrice,value)
model_RMSE

str(df_test$Id)
str(df_test$SalePrice)

# Comparing the model predicted values with observed values
table = data.frame(Id = df_test$Id, SalePrice = value)
dim(table)

write.csv(table,file="C:/Users/dnguy/Desktop/3 Statistical Foundations/Unit 14 & 15 Project/kaggle_forward.csv",row.names = FALSE)

```

## Creating the model using backward variable selection 

Multiple R-Squared=0.9188
Adjusted R-Squared=0.9162
Kaggle RMSE=0.13700
CV Press=19.45764

```{r, eval=FALSE, echo=FALSE}
# Applying backward variable selection method
backward.var=stepAIC(fit.analysis2.model,direction = "backward")
backward.var$anova

# Generating the model with the variables generated by backward variable selection method
back.model=lm(log(SalePrice) ~ MSZoning + log(LotArea) + Street + Alley + LotShape + 
                LandContour + Condition2 + BldgType + OverallQual + OverallCond + 
                YearBuilt + YearRemodAdd + Exterior1st + Exterior2nd + MasVnrType + 
                ExterQual + ExterCond + Foundation + BsmtQual + BsmtExposure + 
                log(BsmtFinSF1) + log(BsmtFinSF2) + log(BsmtUnfSF) + HeatingQC + CentralAir + 
                X1stFlrSF + X2ndFlrSF + LowQualFinSF + BsmtFullBath + FullBath + 
                HalfBath + KitchenAbvGr + KitchenQual + Functional + log(Fireplaces) + 
                FireplaceQu + GarageCars + GarageArea + GarageCond + PavedDrive + 
                log(WoodDeckSF) + log(EnclosedPorch) + ScreenPorch + MiscFeature + 
                YrSold + SaleCondition,data=fit.analysis2)
summary(back.model)

# Calculating the CV Press of the backward linear regression model
ols_press(backward.model)

#Doing the prediction on partitioned test set
prediction=predict(back.model,newdata=test.data)
prediction

#Performing inverse log transform
value=2.718^prediction
value

# Checking the RMSE of the model
rmse.model=rmse(test.data$SalePrice,value)
rmse.model

# Comparing the model predicted values with observed values
table=data.frame(Id=test.data$Id,ObsSalePrice=test.data$SalePrice,PredSalePrice=value)
table

# Predictions on the Original Test Set
predictiontest=predict(back.model,newdata=test)
predictiontest

#Performing inverse log transform
pred_value=2.718^predictiontest
pred_value

# Putting the predicted values in a dataframe 
output.df_train=data.frame(Id=test$Id, SalePrice=pred_value)
head(output.df_train)
dim(output.df_train)
table(is.na(output.df_train))

# Putting the dataframe in a csv to submit on the kaggle to check the Score
write.csv(output.df_train,file="C:/Users/ARTH PATEL/Desktop/MSDS@SMU/6371-LSA/Stats Project/kaggle_submission_backward.csv",row.names = FALSE)
```

## Creating the model using stepwise variable selection 

Multiple R-Squared=0.9188
Adjusted R-Squared=0.9162
Kaggle RMSE=0.13926
CV Press=19.0847

```{r, eval=FALSE, echo=FALSE}
# Applying stepwise variable selection method
stepwise.var=stepAIC(fit.analysis2.model,direction = "both")
stepwise.var$anova

# Generating the model with the variables generated by stepwise variable selection method
step.model=lm(log(SalePrice) ~ MSZoning + log(LotArea) + Street + Alley + LotShape + 
                LandContour + Condition2 + BldgType + OverallQual + OverallCond + 
                YearBuilt + YearRemodAdd + Exterior1st + Exterior2nd + MasVnrType + 
                ExterQual + ExterCond + Foundation + BsmtQual + BsmtExposure + 
                log(BsmtFinSF1) + log(BsmtFinSF2) + log(BsmtUnfSF) + HeatingQC + CentralAir + 
                X1stFlrSF + X2ndFlrSF + LowQualFinSF + BsmtFullBath + FullBath + 
                HalfBath + KitchenAbvGr + KitchenQual + Functional + log(Fireplaces) + 
                FireplaceQu + GarageCars + GarageArea + GarageCond + PavedDrive + 
                log(WoodDeckSF) + log(EnclosedPorch) + ScreenPorch + MiscFeature + 
                YrSold + SaleCondition,data=fit.analysis2)
summary(step.model)

# Calculating the CV Press of the backward linear regression model
ols_press(stepwise.model)

#Doing the prediction on partitioned test set
prediction=predict(step.model,newdata=test.data)
prediction

#Performing inverse log transform
value=2.718^prediction
value

# Checking the RMSE of the model
rmse.model=rmse(test.data$SalePrice,value)
rmse.model

# Comparing the model predicted values with observed values
table=data.frame(Id=test.data$Id,ObsSalePrice=test.data$SalePrice,PredSalePrice=value)
table

# Predictions on the Original Test Set
predictiontest=predict(step.model,newdata=test)
predictiontest

#Performing inverse log transform
pred_value=2.718^predictiontest
pred_value

# Putting the predicted values in a dataframe 
output.df_train=data.frame(Id=test$Id, SalePrice=pred_value)
head(output.df_train)
dim(output.df_train)
table(is.na(output.df_train))

# Putting the dataframe in a csv to submit on the kaggle to check the Score
write.csv(output.df_train,file="C:/Users/ARTH PATEL/Desktop/MSDS@SMU/6371-LSA/Stats Project/kaggle_submission_stepwise.csv",row.names = FALSE)


```

## Creating the model using custom variable selection 

Multiple R-Squared=0.8596
Adjusted R-Squared=0.8568
Kaggle RMSE=0.13926
CV Press=34.379

```{r, eval=FALSE, echo=FALSE}
# Generating the  custom model
custom.model=lm(log(SalePrice) ~ Neighborhood+GarageCars+SaleCondition+RoofStyle+CentralAir+Fireplaces+
                X1stFlrSF*X2ndFlrSF+ScreenPorch+BsmtFinSF1*BsmtFinSF2+KitchenQual+BsmtFullBath*BsmtHalfBath+
                  YearBuilt+PoolQC+HouseStyle+LotArea+BsmtFinType2*BsmtFinType1+BsmtFinType1+Electrical+
                  Electrical*CentralAir+GarageFinish+GarageYrBlt+GarageType,data=fit.analysis2)
summary(custom.model)

# Calculating the CV Press of the custom linear regression model
ols_press(custom.model)


#Doing the prediction on partitioned test set
prediction=predict(custom.model,newdata=test.data)
prediction

#Performing inverse log transform
value=2.718^prediction
value

# Checking the RMSE of the model
rmse.model=rmse(test.data$SalePrice,value)
rmse.model

# Comparing the model predicted values with observed values
table=data.frame(Id=test.data$Id,ObsSalePrice=test.data$SalePrice,PredSalePrice=value)
table

# Predictions on the Original Test Set
predictiontest=predict(custom.model,newdata=test)
predictiontest

#Performing inverse log transform
pred_value=2.718^predictiontest
pred_value

# Putting the predicted values in a dataframe 
output.df_train=data.frame(Id=test$Id, SalePrice=pred_value)
head(output.df_train)
dim(output.df_train)
table(is.na(output.df_train))

# Putting the dataframe in a csv to submit on the kaggle to check the Score
write.csv(output.df_train, file="C:/Users/dnguy/Desktop/3 Statistical Foundations/Unit 14 & 15 Project/kaggle_custom.csv",row.names = FALSE)

```

```{r, eval=FALSE, echo=FALSE}
####Analysis 2 EDA####
plot_histogram(A2Data[,2:79])
```
